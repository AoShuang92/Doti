{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SD_Transformer_TS.ipynb",
      "provenance": [],
      "mount_file_id": "1iuYMoQ6tCsKkhnCoTv46d1eh-r-YrD0j",
      "authorship_tag": "ABX9TyPYEPVo9PAXEBk0r+/Mre8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/Doti/blob/main/SD_Transformer_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIt3FgPPGBdy",
        "outputId": "1cd76aab-70ae-4cd3-e16b-ef53e296626a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "#system\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import json\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from torchtext.legacy import data\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def seed_everything(seed=20):\n",
        "  #random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE-i0XZFDFja",
        "outputId": "d0c70c21-095b-43ce-9e69-dda15036e8df"
      },
      "source": [
        "!pip install \"nltk==3.4.5\"\n",
        "!pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449907 sha256=7fb91471dd16c98ee62ba4817243aafc7e4c6846a9fb547e854cc45d1c372b74\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_IMdSrfIDc1"
      },
      "source": [
        "max_length = 35\n",
        "train_dir = \"/content/drive/MyDrive/calibration_project/medical_dialogue_system/combined_qa_train_ID.csv\"\n",
        "test_dir = \"/content/drive/MyDrive/calibration_project/medical_dialogue_system/combined_qa_test_200_ID.csv\"\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence.lower()\n",
        "\n",
        "def prepare_csv(train,test):\n",
        "    # idx = np.arange(df_train.shape[0])    \n",
        "    # np.random.shuffle(idx)\n",
        "    # val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    train_temp = train[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_train.csv', index=True)\n",
        "    \n",
        "    test_temp = test[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_val.csv', index=True) \n",
        "    return  train_temp,  test_temp\n",
        "\n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size, device=None): \n",
        "    train = pd.read_csv(train_dir,error_bad_lines=False,sep=\",\")\n",
        "    test =  pd.read_csv(test_dir,error_bad_lines=False, sep=\",\")\n",
        "    train['Question'] = train['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    train['Answer'] = train['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Question'] = test['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    \n",
        "    test['Answer'] = test['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    train_temp,  test_temp = prepare_csv(train,test)\n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "\n",
        "    TEXT = data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,batch_first=True, \n",
        "                      fix_length=fix_length)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)  \n",
        "    train_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_train.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "    test_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_val.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "\n",
        "    TEXT.build_vocab(train_temps,test_temps)#, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps, test_temps)\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    print(\"vocab_size_and_ntokens:\",vocab_size,ntokens)\n",
        "    train_loader = get_iterator(train_temps, batch_size=batch_size, \n",
        "                                train=True, shuffle=True,\n",
        "                                repeat=False,device=None)\n",
        "    test_loader = get_iterator(test_temps, batch_size=1, \n",
        "                            train=False, shuffle=False,\n",
        "                            repeat=False, device=None)\n",
        "    print('Train samples:%d'%(len(train_temps)), 'Valid samples:%d'%(len(test_temps)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "            'Valid minibatch nb:%d'%(len(test_loader)))\n",
        "    return vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1gs65-lINWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4706373-2e67-4161-ce62-885c1d3b5283"
      },
      "source": [
        "vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT = get_dataset(fix_length=max_length,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size_and_ntokens: 1938 1938\n",
            "Train samples:1001 Valid samples:200 Train minibatch nb:251 Valid minibatch nb:200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCvd3LSEPRW3"
      },
      "source": [
        "def create_masks(question, reply_input):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDLfGO6I9stZ"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)   \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed_dec(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViltEEHA96TR"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader):\n",
        "\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    all_blue = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            all_blue.append(BLEU_1)\n",
        "    \n",
        "    return np.mean(all_blue)\n",
        "\n",
        "def evaluate(transformer, question, question_mask, max_len):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<sos>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    next_word = -22\n",
        "    while next_word != word_map['<eos>']:\n",
        "    #for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<eos>'] or words.shape[1]==(max_len+1):\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<sos>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "\n",
        "    return sentence\n",
        "\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_meteor = []\n",
        "    all_rouge = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        \n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "            scoress = scorer.score(str(pred_sentence), str(gt))\n",
        "\n",
        "            all_blue1.append(BLEU_1)\n",
        "            all_meteor.append(meteor)\n",
        "            all_rouge.append(scoress['rougeL'][2])\n",
        "           \n",
        "    bleu_score_1 = np.mean(all_blue1)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "    rouge = np.mean(all_rouge)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",bleu_score_1, \"Rouge:\",rouge, \"Meteor:\",met,\"PPL:\",ppl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SUePCc1CUoM"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, inputs, input_mask, targets, targets_mask):\n",
        "        logits = self.model(inputs, input_mask, targets, targets_mask)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size())\n",
        "        return logits / temperature\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        self.cuda()\n",
        "        ece_criterion = _ECELoss().cuda()\n",
        "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for i, pair in enumerate(valid_loader):\n",
        "    \n",
        "                input = pair.Question.cuda()\n",
        "                label = pair.Answer.cuda()\n",
        "            #for input, label in valid_loader:\n",
        "                input = input.cuda()\n",
        "                label = label.cuda()\n",
        "                label = label[:, 1:]\n",
        "                input_mask, label_mask = create_masks(input, label)\n",
        "                logits = self.model(input, input_mask, label, label_mask)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).cuda()\n",
        "            labels = torch.cat(labels_list).cuda()\n",
        "            \n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        init_temp = self.temperature.clone()\n",
        "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "        def eval():\n",
        "            labels_loss = labels.reshape(-1)\n",
        "            loss = nll_criterion(self.temperature_scale(logits.view(-1, ntokens)), labels_loss)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(eval)\n",
        "\n",
        "        # CalculateECE after temperature scaling\n",
        "        labels_loss = labels.reshape(-1)\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits.view(-1,ntokens )), labels_loss).item()\n",
        "        print('Initial temperature: %.3f, Optimal temperature: %.3f' % (init_temp, self.temperature.item()))\n",
        "        #print('Initial temperature: %.3f, Optimal temperature: %.3f' % (init_temp, after_temperature_ece))\n",
        "        return self\n",
        "        \n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    \n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            \n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            \n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                all_blue.append(BLEU_1)\n",
        "\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4leW_w979N7"
      },
      "source": [
        "getting optimal T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT-Tww9Z6Vx8",
        "outputId": "79cbd7fa-ab5b-4a9f-fe1b-32a135357111"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('Before TS- bleu:%.3f, bef ece:%.5f'%(bleu,temperature_ece))\n",
        "\n",
        "model_ts = ModelWithTemperature(model)\n",
        "model_ts.set_temperature(test_loader)\n",
        "bleu, logits_all, labels_all = evaluation(model_ts, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('After TS- bleu:%.3f,aft ece:%.5f'%(bleu,temperature_ece))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before TS- bleu:0.405, bef ece:0.38368\n",
            "Initial temperature: 1.500, Optimal temperature: 5.025\n",
            "After TS- bleu:0.405,aft ece:0.33059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVhMGbct8C8S"
      },
      "source": [
        "Training with SD, T = 1.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAce1oEn6oGJ",
        "outputId": "5c6ba977-c59f-4560-a79c-e439c580b036"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "adam_optimizer = torch.optim.Adam(model_s.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model_t.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth'))\n",
        "\n",
        "T = 1.5\n",
        "criterionKD = SoftTarget(T)\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    \n",
        "    train_sd(train_loader, model_t, model_s, criterion, criterionKD, transformer_optimizer, epoch)\n",
        "    blue_score = valid (test_loader, model_s)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_s.state_dict(), '/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_1.5t.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00153, best epoch:0, best blue:0.00153\n",
            "cur epoch:1, cur blue:0.09313, best epoch:1, best blue:0.09313\n",
            "cur epoch:2, cur blue:0.15070, best epoch:2, best blue:0.15070\n",
            "cur epoch:3, cur blue:0.17288, best epoch:3, best blue:0.17288\n",
            "cur epoch:4, cur blue:0.21768, best epoch:4, best blue:0.21768\n",
            "cur epoch:5, cur blue:0.24554, best epoch:5, best blue:0.24554\n",
            "cur epoch:6, cur blue:0.26875, best epoch:6, best blue:0.26875\n",
            "cur epoch:7, cur blue:0.32112, best epoch:7, best blue:0.32112\n",
            "cur epoch:8, cur blue:0.33757, best epoch:8, best blue:0.33757\n",
            "cur epoch:9, cur blue:0.34034, best epoch:9, best blue:0.34034\n",
            "cur epoch:10, cur blue:0.35039, best epoch:10, best blue:0.35039\n",
            "cur epoch:11, cur blue:0.35494, best epoch:11, best blue:0.35494\n",
            "cur epoch:12, cur blue:0.33527, best epoch:11, best blue:0.35494\n",
            "cur epoch:13, cur blue:0.35988, best epoch:13, best blue:0.35988\n",
            "cur epoch:14, cur blue:0.34959, best epoch:13, best blue:0.35988\n",
            "cur epoch:15, cur blue:0.32483, best epoch:13, best blue:0.35988\n",
            "cur epoch:16, cur blue:0.32096, best epoch:13, best blue:0.35988\n",
            "cur epoch:17, cur blue:0.35898, best epoch:13, best blue:0.35988\n",
            "cur epoch:18, cur blue:0.36014, best epoch:18, best blue:0.36014\n",
            "cur epoch:19, cur blue:0.36066, best epoch:19, best blue:0.36066\n",
            "cur epoch:20, cur blue:0.36953, best epoch:20, best blue:0.36953\n",
            "cur epoch:21, cur blue:0.36184, best epoch:20, best blue:0.36953\n",
            "cur epoch:22, cur blue:0.37638, best epoch:22, best blue:0.37638\n",
            "cur epoch:23, cur blue:0.39312, best epoch:23, best blue:0.39312\n",
            "cur epoch:24, cur blue:0.39680, best epoch:24, best blue:0.39680\n",
            "cur epoch:25, cur blue:0.39827, best epoch:25, best blue:0.39827\n",
            "cur epoch:26, cur blue:0.40010, best epoch:26, best blue:0.40010\n",
            "cur epoch:27, cur blue:0.40807, best epoch:27, best blue:0.40807\n",
            "cur epoch:28, cur blue:0.40040, best epoch:27, best blue:0.40807\n",
            "cur epoch:29, cur blue:0.39428, best epoch:27, best blue:0.40807\n",
            "cur epoch:30, cur blue:0.39804, best epoch:27, best blue:0.40807\n",
            "cur epoch:31, cur blue:0.40444, best epoch:27, best blue:0.40807\n",
            "cur epoch:32, cur blue:0.39932, best epoch:27, best blue:0.40807\n",
            "cur epoch:33, cur blue:0.41176, best epoch:33, best blue:0.41176\n",
            "cur epoch:34, cur blue:0.41646, best epoch:34, best blue:0.41646\n",
            "cur epoch:35, cur blue:0.40857, best epoch:34, best blue:0.41646\n",
            "cur epoch:36, cur blue:0.40975, best epoch:34, best blue:0.41646\n",
            "cur epoch:37, cur blue:0.41514, best epoch:34, best blue:0.41646\n",
            "cur epoch:38, cur blue:0.41110, best epoch:34, best blue:0.41646\n",
            "cur epoch:39, cur blue:0.41948, best epoch:39, best blue:0.41948\n",
            "cur epoch:40, cur blue:0.40965, best epoch:39, best blue:0.41948\n",
            "cur epoch:41, cur blue:0.41004, best epoch:39, best blue:0.41948\n",
            "cur epoch:42, cur blue:0.41966, best epoch:42, best blue:0.41966\n",
            "cur epoch:43, cur blue:0.41315, best epoch:42, best blue:0.41966\n",
            "cur epoch:44, cur blue:0.41550, best epoch:42, best blue:0.41966\n",
            "cur epoch:45, cur blue:0.41483, best epoch:42, best blue:0.41966\n",
            "cur epoch:46, cur blue:0.41614, best epoch:42, best blue:0.41966\n",
            "cur epoch:47, cur blue:0.41562, best epoch:42, best blue:0.41966\n",
            "cur epoch:48, cur blue:0.43241, best epoch:48, best blue:0.43241\n",
            "cur epoch:49, cur blue:0.41876, best epoch:48, best blue:0.43241\n",
            "cur epoch:50, cur blue:0.42204, best epoch:48, best blue:0.43241\n",
            "cur epoch:51, cur blue:0.42206, best epoch:48, best blue:0.43241\n",
            "cur epoch:52, cur blue:0.42172, best epoch:48, best blue:0.43241\n",
            "cur epoch:53, cur blue:0.42626, best epoch:48, best blue:0.43241\n",
            "cur epoch:54, cur blue:0.42948, best epoch:48, best blue:0.43241\n",
            "cur epoch:55, cur blue:0.43020, best epoch:48, best blue:0.43241\n",
            "cur epoch:56, cur blue:0.43070, best epoch:48, best blue:0.43241\n",
            "cur epoch:57, cur blue:0.42279, best epoch:48, best blue:0.43241\n",
            "cur epoch:58, cur blue:0.41826, best epoch:48, best blue:0.43241\n",
            "cur epoch:59, cur blue:0.42123, best epoch:48, best blue:0.43241\n",
            "cur epoch:60, cur blue:0.42663, best epoch:48, best blue:0.43241\n",
            "cur epoch:61, cur blue:0.42617, best epoch:48, best blue:0.43241\n",
            "cur epoch:62, cur blue:0.42627, best epoch:48, best blue:0.43241\n",
            "cur epoch:63, cur blue:0.43197, best epoch:48, best blue:0.43241\n",
            "cur epoch:64, cur blue:0.42297, best epoch:48, best blue:0.43241\n",
            "cur epoch:65, cur blue:0.43007, best epoch:48, best blue:0.43241\n",
            "cur epoch:66, cur blue:0.43003, best epoch:48, best blue:0.43241\n",
            "cur epoch:67, cur blue:0.42999, best epoch:48, best blue:0.43241\n",
            "cur epoch:68, cur blue:0.42570, best epoch:48, best blue:0.43241\n",
            "cur epoch:69, cur blue:0.42964, best epoch:48, best blue:0.43241\n",
            "cur epoch:70, cur blue:0.42822, best epoch:48, best blue:0.43241\n",
            "cur epoch:71, cur blue:0.43072, best epoch:48, best blue:0.43241\n",
            "cur epoch:72, cur blue:0.42565, best epoch:48, best blue:0.43241\n",
            "cur epoch:73, cur blue:0.42864, best epoch:48, best blue:0.43241\n",
            "cur epoch:74, cur blue:0.42245, best epoch:48, best blue:0.43241\n",
            "cur epoch:75, cur blue:0.43199, best epoch:48, best blue:0.43241\n",
            "cur epoch:76, cur blue:0.41822, best epoch:48, best blue:0.43241\n",
            "cur epoch:77, cur blue:0.43299, best epoch:77, best blue:0.43299\n",
            "cur epoch:78, cur blue:0.42641, best epoch:77, best blue:0.43299\n",
            "cur epoch:79, cur blue:0.42518, best epoch:77, best blue:0.43299\n",
            "cur epoch:80, cur blue:0.43179, best epoch:77, best blue:0.43299\n",
            "cur epoch:81, cur blue:0.43214, best epoch:77, best blue:0.43299\n",
            "cur epoch:82, cur blue:0.42741, best epoch:77, best blue:0.43299\n",
            "cur epoch:83, cur blue:0.43199, best epoch:77, best blue:0.43299\n",
            "cur epoch:84, cur blue:0.42050, best epoch:77, best blue:0.43299\n",
            "cur epoch:85, cur blue:0.43076, best epoch:77, best blue:0.43299\n",
            "cur epoch:86, cur blue:0.43220, best epoch:77, best blue:0.43299\n",
            "cur epoch:87, cur blue:0.43312, best epoch:87, best blue:0.43312\n",
            "cur epoch:88, cur blue:0.43618, best epoch:88, best blue:0.43618\n",
            "cur epoch:89, cur blue:0.42930, best epoch:88, best blue:0.43618\n",
            "cur epoch:90, cur blue:0.42532, best epoch:88, best blue:0.43618\n",
            "cur epoch:91, cur blue:0.42968, best epoch:88, best blue:0.43618\n",
            "cur epoch:92, cur blue:0.41778, best epoch:88, best blue:0.43618\n",
            "cur epoch:93, cur blue:0.42925, best epoch:88, best blue:0.43618\n",
            "cur epoch:94, cur blue:0.43481, best epoch:88, best blue:0.43618\n",
            "cur epoch:95, cur blue:0.41915, best epoch:88, best blue:0.43618\n",
            "cur epoch:96, cur blue:0.43663, best epoch:96, best blue:0.43663\n",
            "cur epoch:97, cur blue:0.42606, best epoch:96, best blue:0.43663\n",
            "cur epoch:98, cur blue:0.43419, best epoch:96, best blue:0.43663\n",
            "cur epoch:99, cur blue:0.43821, best epoch:99, best blue:0.43821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRLFSnht6oPy",
        "outputId": "ee14f3d5-6f91-4739-9a35-3185bb5af624"
      },
      "source": [
        " from rouge_score import rouge_scorer\n",
        "evaluate_matrics(model_s,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.43820982960671756 Rouge: 0.45582540667987315 Meteor: 0.4231837980578683 PPL: 10.35602329891361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnUz2J2BAsHU"
      },
      "source": [
        "Training with SD, T=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsjPLo5J_o5o",
        "outputId": "865e71cb-9b62-4aac-f0aa-077c0a1a77f5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t2 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s2 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "adam_optimizer = torch.optim.Adam(model_s2.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model_t2.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth'))\n",
        "\n",
        "T = 2\n",
        "criterionKD = SoftTarget(T)\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    \n",
        "    train_sd(train_loader, model_t2, model_s2, criterion, criterionKD, transformer_optimizer, epoch)\n",
        "    blue_score = valid (test_loader, model_s2)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_s2.state_dict(), '/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_2t.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00002, best epoch:0, best blue:0.00002\n",
            "cur epoch:1, cur blue:0.11241, best epoch:1, best blue:0.11241\n",
            "cur epoch:2, cur blue:0.14806, best epoch:2, best blue:0.14806\n",
            "cur epoch:3, cur blue:0.16120, best epoch:3, best blue:0.16120\n",
            "cur epoch:4, cur blue:0.18181, best epoch:4, best blue:0.18181\n",
            "cur epoch:5, cur blue:0.20640, best epoch:5, best blue:0.20640\n",
            "cur epoch:6, cur blue:0.27457, best epoch:6, best blue:0.27457\n",
            "cur epoch:7, cur blue:0.28708, best epoch:7, best blue:0.28708\n",
            "cur epoch:8, cur blue:0.31303, best epoch:8, best blue:0.31303\n",
            "cur epoch:9, cur blue:0.34107, best epoch:9, best blue:0.34107\n",
            "cur epoch:10, cur blue:0.31539, best epoch:9, best blue:0.34107\n",
            "cur epoch:11, cur blue:0.31423, best epoch:9, best blue:0.34107\n",
            "cur epoch:12, cur blue:0.33945, best epoch:9, best blue:0.34107\n",
            "cur epoch:13, cur blue:0.34015, best epoch:9, best blue:0.34107\n",
            "cur epoch:14, cur blue:0.35060, best epoch:14, best blue:0.35060\n",
            "cur epoch:15, cur blue:0.33898, best epoch:14, best blue:0.35060\n",
            "cur epoch:16, cur blue:0.34690, best epoch:14, best blue:0.35060\n",
            "cur epoch:17, cur blue:0.34678, best epoch:14, best blue:0.35060\n",
            "cur epoch:18, cur blue:0.37280, best epoch:18, best blue:0.37280\n",
            "cur epoch:19, cur blue:0.36744, best epoch:18, best blue:0.37280\n",
            "cur epoch:20, cur blue:0.38632, best epoch:20, best blue:0.38632\n",
            "cur epoch:21, cur blue:0.40313, best epoch:21, best blue:0.40313\n",
            "cur epoch:22, cur blue:0.39267, best epoch:21, best blue:0.40313\n",
            "cur epoch:23, cur blue:0.41570, best epoch:23, best blue:0.41570\n",
            "cur epoch:24, cur blue:0.38318, best epoch:23, best blue:0.41570\n",
            "cur epoch:25, cur blue:0.40208, best epoch:23, best blue:0.41570\n",
            "cur epoch:26, cur blue:0.40231, best epoch:23, best blue:0.41570\n",
            "cur epoch:27, cur blue:0.40563, best epoch:23, best blue:0.41570\n",
            "cur epoch:28, cur blue:0.39351, best epoch:23, best blue:0.41570\n",
            "cur epoch:29, cur blue:0.40989, best epoch:23, best blue:0.41570\n",
            "cur epoch:30, cur blue:0.41074, best epoch:23, best blue:0.41570\n",
            "cur epoch:31, cur blue:0.41279, best epoch:23, best blue:0.41570\n",
            "cur epoch:32, cur blue:0.41428, best epoch:23, best blue:0.41570\n",
            "cur epoch:33, cur blue:0.43628, best epoch:33, best blue:0.43628\n",
            "cur epoch:34, cur blue:0.41825, best epoch:33, best blue:0.43628\n",
            "cur epoch:35, cur blue:0.40512, best epoch:33, best blue:0.43628\n",
            "cur epoch:36, cur blue:0.41906, best epoch:33, best blue:0.43628\n",
            "cur epoch:37, cur blue:0.43067, best epoch:33, best blue:0.43628\n",
            "cur epoch:38, cur blue:0.42986, best epoch:33, best blue:0.43628\n",
            "cur epoch:39, cur blue:0.41729, best epoch:33, best blue:0.43628\n",
            "cur epoch:40, cur blue:0.41994, best epoch:33, best blue:0.43628\n",
            "cur epoch:41, cur blue:0.40538, best epoch:33, best blue:0.43628\n",
            "cur epoch:42, cur blue:0.42370, best epoch:33, best blue:0.43628\n",
            "cur epoch:43, cur blue:0.42119, best epoch:33, best blue:0.43628\n",
            "cur epoch:44, cur blue:0.42658, best epoch:33, best blue:0.43628\n",
            "cur epoch:45, cur blue:0.42576, best epoch:33, best blue:0.43628\n",
            "cur epoch:46, cur blue:0.43917, best epoch:46, best blue:0.43917\n",
            "cur epoch:47, cur blue:0.42732, best epoch:46, best blue:0.43917\n",
            "cur epoch:48, cur blue:0.43898, best epoch:46, best blue:0.43917\n",
            "cur epoch:49, cur blue:0.43199, best epoch:46, best blue:0.43917\n",
            "cur epoch:50, cur blue:0.42650, best epoch:46, best blue:0.43917\n",
            "cur epoch:51, cur blue:0.42882, best epoch:46, best blue:0.43917\n",
            "cur epoch:52, cur blue:0.42882, best epoch:46, best blue:0.43917\n",
            "cur epoch:53, cur blue:0.42949, best epoch:46, best blue:0.43917\n",
            "cur epoch:54, cur blue:0.44352, best epoch:54, best blue:0.44352\n",
            "cur epoch:55, cur blue:0.42937, best epoch:54, best blue:0.44352\n",
            "cur epoch:56, cur blue:0.42595, best epoch:54, best blue:0.44352\n",
            "cur epoch:57, cur blue:0.42663, best epoch:54, best blue:0.44352\n",
            "cur epoch:58, cur blue:0.42201, best epoch:54, best blue:0.44352\n",
            "cur epoch:59, cur blue:0.43829, best epoch:54, best blue:0.44352\n",
            "cur epoch:60, cur blue:0.43244, best epoch:54, best blue:0.44352\n",
            "cur epoch:61, cur blue:0.43353, best epoch:54, best blue:0.44352\n",
            "cur epoch:62, cur blue:0.42280, best epoch:54, best blue:0.44352\n",
            "cur epoch:63, cur blue:0.43582, best epoch:54, best blue:0.44352\n",
            "cur epoch:64, cur blue:0.42918, best epoch:54, best blue:0.44352\n",
            "cur epoch:65, cur blue:0.43550, best epoch:54, best blue:0.44352\n",
            "cur epoch:66, cur blue:0.43825, best epoch:54, best blue:0.44352\n",
            "cur epoch:67, cur blue:0.43434, best epoch:54, best blue:0.44352\n",
            "cur epoch:68, cur blue:0.42971, best epoch:54, best blue:0.44352\n",
            "cur epoch:69, cur blue:0.44112, best epoch:54, best blue:0.44352\n",
            "cur epoch:70, cur blue:0.43461, best epoch:54, best blue:0.44352\n",
            "cur epoch:71, cur blue:0.42980, best epoch:54, best blue:0.44352\n",
            "cur epoch:72, cur blue:0.43617, best epoch:54, best blue:0.44352\n",
            "cur epoch:73, cur blue:0.43908, best epoch:54, best blue:0.44352\n",
            "cur epoch:74, cur blue:0.43246, best epoch:54, best blue:0.44352\n",
            "cur epoch:75, cur blue:0.43738, best epoch:54, best blue:0.44352\n",
            "cur epoch:76, cur blue:0.43713, best epoch:54, best blue:0.44352\n",
            "cur epoch:77, cur blue:0.43487, best epoch:54, best blue:0.44352\n",
            "cur epoch:78, cur blue:0.43265, best epoch:54, best blue:0.44352\n",
            "cur epoch:79, cur blue:0.42607, best epoch:54, best blue:0.44352\n",
            "cur epoch:80, cur blue:0.43158, best epoch:54, best blue:0.44352\n",
            "cur epoch:81, cur blue:0.43110, best epoch:54, best blue:0.44352\n",
            "cur epoch:82, cur blue:0.43940, best epoch:54, best blue:0.44352\n",
            "cur epoch:83, cur blue:0.43182, best epoch:54, best blue:0.44352\n",
            "cur epoch:84, cur blue:0.43074, best epoch:54, best blue:0.44352\n",
            "cur epoch:85, cur blue:0.42499, best epoch:54, best blue:0.44352\n",
            "cur epoch:86, cur blue:0.43545, best epoch:54, best blue:0.44352\n",
            "cur epoch:87, cur blue:0.43401, best epoch:54, best blue:0.44352\n",
            "cur epoch:88, cur blue:0.43259, best epoch:54, best blue:0.44352\n",
            "cur epoch:89, cur blue:0.42826, best epoch:54, best blue:0.44352\n",
            "cur epoch:90, cur blue:0.43508, best epoch:54, best blue:0.44352\n",
            "cur epoch:91, cur blue:0.43693, best epoch:54, best blue:0.44352\n",
            "cur epoch:92, cur blue:0.42939, best epoch:54, best blue:0.44352\n",
            "cur epoch:93, cur blue:0.44329, best epoch:54, best blue:0.44352\n",
            "cur epoch:94, cur blue:0.43938, best epoch:54, best blue:0.44352\n",
            "cur epoch:95, cur blue:0.44158, best epoch:54, best blue:0.44352\n",
            "cur epoch:96, cur blue:0.43285, best epoch:54, best blue:0.44352\n",
            "cur epoch:97, cur blue:0.44771, best epoch:97, best blue:0.44771\n",
            "cur epoch:98, cur blue:0.44147, best epoch:97, best blue:0.44771\n",
            "cur epoch:99, cur blue:0.43777, best epoch:97, best blue:0.44771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppVKr2Nh_pFE",
        "outputId": "deeed0e7-f345-4fc1-88a9-84e4713b4609"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "model_t2 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s2 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s2.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_2t.pth'))\n",
        "evaluate_matrics(model_s2,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.4477079312147275 Rouge: 0.46757194816556946 Meteor: 0.42746513264813046 PPL: 10.15113923168798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23yLXWgZFgox"
      },
      "source": [
        "Training with SD, T=3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0jA1-wV_pO9",
        "outputId": "995b3e4b-7c1b-443a-c380-75aa2ba6a09a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t3 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s3 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "adam_optimizer = torch.optim.Adam(model_s3.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model_t3.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth'))\n",
        "\n",
        "T = 3\n",
        "criterionKD = SoftTarget(T)\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    \n",
        "    train_sd(train_loader, model_t3, model_s3, criterion, criterionKD, transformer_optimizer, epoch)\n",
        "    blue_score = valid (test_loader, model_s3)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_s3.state_dict(), '/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_3t.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00000, best epoch:0, best blue:0.00000\n",
            "cur epoch:1, cur blue:0.11092, best epoch:1, best blue:0.11092\n",
            "cur epoch:2, cur blue:0.13236, best epoch:2, best blue:0.13236\n",
            "cur epoch:3, cur blue:0.15837, best epoch:3, best blue:0.15837\n",
            "cur epoch:4, cur blue:0.18281, best epoch:4, best blue:0.18281\n",
            "cur epoch:5, cur blue:0.22478, best epoch:5, best blue:0.22478\n",
            "cur epoch:6, cur blue:0.25175, best epoch:6, best blue:0.25175\n",
            "cur epoch:7, cur blue:0.26997, best epoch:7, best blue:0.26997\n",
            "cur epoch:8, cur blue:0.29078, best epoch:8, best blue:0.29078\n",
            "cur epoch:9, cur blue:0.32251, best epoch:9, best blue:0.32251\n",
            "cur epoch:10, cur blue:0.31240, best epoch:9, best blue:0.32251\n",
            "cur epoch:11, cur blue:0.33043, best epoch:11, best blue:0.33043\n",
            "cur epoch:12, cur blue:0.32563, best epoch:11, best blue:0.33043\n",
            "cur epoch:13, cur blue:0.34783, best epoch:13, best blue:0.34783\n",
            "cur epoch:14, cur blue:0.31138, best epoch:13, best blue:0.34783\n",
            "cur epoch:15, cur blue:0.32509, best epoch:13, best blue:0.34783\n",
            "cur epoch:16, cur blue:0.32835, best epoch:13, best blue:0.34783\n",
            "cur epoch:17, cur blue:0.35598, best epoch:17, best blue:0.35598\n",
            "cur epoch:18, cur blue:0.34939, best epoch:17, best blue:0.35598\n",
            "cur epoch:19, cur blue:0.38254, best epoch:19, best blue:0.38254\n",
            "cur epoch:20, cur blue:0.36790, best epoch:19, best blue:0.38254\n",
            "cur epoch:21, cur blue:0.39124, best epoch:21, best blue:0.39124\n",
            "cur epoch:22, cur blue:0.37912, best epoch:21, best blue:0.39124\n",
            "cur epoch:23, cur blue:0.40387, best epoch:23, best blue:0.40387\n",
            "cur epoch:24, cur blue:0.39134, best epoch:23, best blue:0.40387\n",
            "cur epoch:25, cur blue:0.39984, best epoch:23, best blue:0.40387\n",
            "cur epoch:26, cur blue:0.39505, best epoch:23, best blue:0.40387\n",
            "cur epoch:27, cur blue:0.41890, best epoch:27, best blue:0.41890\n",
            "cur epoch:28, cur blue:0.41007, best epoch:27, best blue:0.41890\n",
            "cur epoch:29, cur blue:0.40334, best epoch:27, best blue:0.41890\n",
            "cur epoch:30, cur blue:0.40201, best epoch:27, best blue:0.41890\n",
            "cur epoch:31, cur blue:0.41139, best epoch:27, best blue:0.41890\n",
            "cur epoch:32, cur blue:0.41846, best epoch:27, best blue:0.41890\n",
            "cur epoch:33, cur blue:0.42269, best epoch:33, best blue:0.42269\n",
            "cur epoch:34, cur blue:0.42524, best epoch:34, best blue:0.42524\n",
            "cur epoch:35, cur blue:0.42788, best epoch:35, best blue:0.42788\n",
            "cur epoch:36, cur blue:0.41855, best epoch:35, best blue:0.42788\n",
            "cur epoch:37, cur blue:0.43425, best epoch:37, best blue:0.43425\n",
            "cur epoch:38, cur blue:0.42223, best epoch:37, best blue:0.43425\n",
            "cur epoch:39, cur blue:0.42544, best epoch:37, best blue:0.43425\n",
            "cur epoch:40, cur blue:0.42164, best epoch:37, best blue:0.43425\n",
            "cur epoch:41, cur blue:0.43228, best epoch:37, best blue:0.43425\n",
            "cur epoch:42, cur blue:0.42007, best epoch:37, best blue:0.43425\n",
            "cur epoch:43, cur blue:0.42938, best epoch:37, best blue:0.43425\n",
            "cur epoch:44, cur blue:0.42180, best epoch:37, best blue:0.43425\n",
            "cur epoch:45, cur blue:0.43393, best epoch:37, best blue:0.43425\n",
            "cur epoch:46, cur blue:0.42697, best epoch:37, best blue:0.43425\n",
            "cur epoch:47, cur blue:0.42117, best epoch:37, best blue:0.43425\n",
            "cur epoch:48, cur blue:0.43600, best epoch:48, best blue:0.43600\n",
            "cur epoch:49, cur blue:0.43712, best epoch:49, best blue:0.43712\n",
            "cur epoch:50, cur blue:0.43445, best epoch:49, best blue:0.43712\n",
            "cur epoch:51, cur blue:0.43856, best epoch:51, best blue:0.43856\n",
            "cur epoch:52, cur blue:0.42722, best epoch:51, best blue:0.43856\n",
            "cur epoch:53, cur blue:0.44650, best epoch:53, best blue:0.44650\n",
            "cur epoch:54, cur blue:0.43685, best epoch:53, best blue:0.44650\n",
            "cur epoch:55, cur blue:0.42768, best epoch:53, best blue:0.44650\n",
            "cur epoch:56, cur blue:0.42755, best epoch:53, best blue:0.44650\n",
            "cur epoch:57, cur blue:0.42603, best epoch:53, best blue:0.44650\n",
            "cur epoch:58, cur blue:0.42773, best epoch:53, best blue:0.44650\n",
            "cur epoch:59, cur blue:0.43942, best epoch:53, best blue:0.44650\n",
            "cur epoch:60, cur blue:0.42950, best epoch:53, best blue:0.44650\n",
            "cur epoch:61, cur blue:0.43455, best epoch:53, best blue:0.44650\n",
            "cur epoch:62, cur blue:0.42249, best epoch:53, best blue:0.44650\n",
            "cur epoch:63, cur blue:0.43124, best epoch:53, best blue:0.44650\n",
            "cur epoch:64, cur blue:0.43778, best epoch:53, best blue:0.44650\n",
            "cur epoch:65, cur blue:0.43059, best epoch:53, best blue:0.44650\n",
            "cur epoch:66, cur blue:0.42946, best epoch:53, best blue:0.44650\n",
            "cur epoch:67, cur blue:0.43201, best epoch:53, best blue:0.44650\n",
            "cur epoch:68, cur blue:0.43117, best epoch:53, best blue:0.44650\n",
            "cur epoch:69, cur blue:0.41786, best epoch:53, best blue:0.44650\n",
            "cur epoch:70, cur blue:0.42755, best epoch:53, best blue:0.44650\n",
            "cur epoch:71, cur blue:0.43457, best epoch:53, best blue:0.44650\n",
            "cur epoch:72, cur blue:0.43367, best epoch:53, best blue:0.44650\n",
            "cur epoch:73, cur blue:0.42881, best epoch:53, best blue:0.44650\n",
            "cur epoch:74, cur blue:0.42928, best epoch:53, best blue:0.44650\n",
            "cur epoch:75, cur blue:0.42631, best epoch:53, best blue:0.44650\n",
            "cur epoch:76, cur blue:0.42898, best epoch:53, best blue:0.44650\n",
            "cur epoch:77, cur blue:0.42948, best epoch:53, best blue:0.44650\n",
            "cur epoch:78, cur blue:0.42584, best epoch:53, best blue:0.44650\n",
            "cur epoch:79, cur blue:0.42751, best epoch:53, best blue:0.44650\n",
            "cur epoch:80, cur blue:0.42846, best epoch:53, best blue:0.44650\n",
            "cur epoch:81, cur blue:0.43304, best epoch:53, best blue:0.44650\n",
            "cur epoch:82, cur blue:0.43827, best epoch:53, best blue:0.44650\n",
            "cur epoch:83, cur blue:0.43034, best epoch:53, best blue:0.44650\n",
            "cur epoch:84, cur blue:0.42557, best epoch:53, best blue:0.44650\n",
            "cur epoch:85, cur blue:0.42546, best epoch:53, best blue:0.44650\n",
            "cur epoch:86, cur blue:0.43084, best epoch:53, best blue:0.44650\n",
            "cur epoch:87, cur blue:0.43060, best epoch:53, best blue:0.44650\n",
            "cur epoch:88, cur blue:0.43938, best epoch:53, best blue:0.44650\n",
            "cur epoch:89, cur blue:0.43643, best epoch:53, best blue:0.44650\n",
            "cur epoch:90, cur blue:0.43362, best epoch:53, best blue:0.44650\n",
            "cur epoch:91, cur blue:0.43165, best epoch:53, best blue:0.44650\n",
            "cur epoch:92, cur blue:0.43549, best epoch:53, best blue:0.44650\n",
            "cur epoch:93, cur blue:0.42888, best epoch:53, best blue:0.44650\n",
            "cur epoch:94, cur blue:0.42759, best epoch:53, best blue:0.44650\n",
            "cur epoch:95, cur blue:0.43380, best epoch:53, best blue:0.44650\n",
            "cur epoch:96, cur blue:0.43790, best epoch:53, best blue:0.44650\n",
            "cur epoch:97, cur blue:0.44192, best epoch:53, best blue:0.44650\n",
            "cur epoch:98, cur blue:0.43207, best epoch:53, best blue:0.44650\n",
            "cur epoch:99, cur blue:0.43738, best epoch:53, best blue:0.44650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Weib1NS_FrD7",
        "outputId": "8b59d12f-cc92-49fb-b10e-b7af1aa3040e"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "model_t3 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s3 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s3.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_3t.pth'))\n",
        "evaluate_matrics(model_s3,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.4465045968261427 Rouge: 0.46306013556599646 Meteor: 0.432026245843939 PPL: 10.197486452813704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3OqlBH2HH39"
      },
      "source": [
        "Traning with SD, T=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMD0dVZCFrGl",
        "outputId": "c77d11a7-55e1-45c1-e531-3e375337ee49"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t4 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s4 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "adam_optimizer = torch.optim.Adam(model_s4.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model_t4.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth'))\n",
        "\n",
        "T = 4\n",
        "criterionKD = SoftTarget(T)\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    \n",
        "    train_sd(train_loader, model_t4, model_s4, criterion, criterionKD, transformer_optimizer, epoch)\n",
        "    blue_score = valid (test_loader, model_s4)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_s4.state_dict(), '/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_4t.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00000, best epoch:0, best blue:0.00000\n",
            "cur epoch:1, cur blue:0.09470, best epoch:1, best blue:0.09470\n",
            "cur epoch:2, cur blue:0.13577, best epoch:2, best blue:0.13577\n",
            "cur epoch:3, cur blue:0.15288, best epoch:3, best blue:0.15288\n",
            "cur epoch:4, cur blue:0.17612, best epoch:4, best blue:0.17612\n",
            "cur epoch:5, cur blue:0.22725, best epoch:5, best blue:0.22725\n",
            "cur epoch:6, cur blue:0.24488, best epoch:6, best blue:0.24488\n",
            "cur epoch:7, cur blue:0.27504, best epoch:7, best blue:0.27504\n",
            "cur epoch:8, cur blue:0.29651, best epoch:8, best blue:0.29651\n",
            "cur epoch:9, cur blue:0.32312, best epoch:9, best blue:0.32312\n",
            "cur epoch:10, cur blue:0.30938, best epoch:9, best blue:0.32312\n",
            "cur epoch:11, cur blue:0.33677, best epoch:11, best blue:0.33677\n",
            "cur epoch:12, cur blue:0.34729, best epoch:12, best blue:0.34729\n",
            "cur epoch:13, cur blue:0.34584, best epoch:12, best blue:0.34729\n",
            "cur epoch:14, cur blue:0.32401, best epoch:12, best blue:0.34729\n",
            "cur epoch:15, cur blue:0.29179, best epoch:12, best blue:0.34729\n",
            "cur epoch:16, cur blue:0.31489, best epoch:12, best blue:0.34729\n",
            "cur epoch:17, cur blue:0.36074, best epoch:17, best blue:0.36074\n",
            "cur epoch:18, cur blue:0.35968, best epoch:17, best blue:0.36074\n",
            "cur epoch:19, cur blue:0.37779, best epoch:19, best blue:0.37779\n",
            "cur epoch:20, cur blue:0.36869, best epoch:19, best blue:0.37779\n",
            "cur epoch:21, cur blue:0.37322, best epoch:19, best blue:0.37779\n",
            "cur epoch:22, cur blue:0.38283, best epoch:22, best blue:0.38283\n",
            "cur epoch:23, cur blue:0.39594, best epoch:23, best blue:0.39594\n",
            "cur epoch:24, cur blue:0.39645, best epoch:24, best blue:0.39645\n",
            "cur epoch:25, cur blue:0.40160, best epoch:25, best blue:0.40160\n",
            "cur epoch:26, cur blue:0.40377, best epoch:26, best blue:0.40377\n",
            "cur epoch:27, cur blue:0.41020, best epoch:27, best blue:0.41020\n",
            "cur epoch:28, cur blue:0.41632, best epoch:28, best blue:0.41632\n",
            "cur epoch:29, cur blue:0.40845, best epoch:28, best blue:0.41632\n",
            "cur epoch:30, cur blue:0.42088, best epoch:30, best blue:0.42088\n",
            "cur epoch:31, cur blue:0.41588, best epoch:30, best blue:0.42088\n",
            "cur epoch:32, cur blue:0.41282, best epoch:30, best blue:0.42088\n",
            "cur epoch:33, cur blue:0.40839, best epoch:30, best blue:0.42088\n",
            "cur epoch:34, cur blue:0.41680, best epoch:30, best blue:0.42088\n",
            "cur epoch:35, cur blue:0.41143, best epoch:30, best blue:0.42088\n",
            "cur epoch:36, cur blue:0.41934, best epoch:30, best blue:0.42088\n",
            "cur epoch:37, cur blue:0.42483, best epoch:37, best blue:0.42483\n",
            "cur epoch:38, cur blue:0.41729, best epoch:37, best blue:0.42483\n",
            "cur epoch:39, cur blue:0.43051, best epoch:39, best blue:0.43051\n",
            "cur epoch:40, cur blue:0.42110, best epoch:39, best blue:0.43051\n",
            "cur epoch:41, cur blue:0.41603, best epoch:39, best blue:0.43051\n",
            "cur epoch:42, cur blue:0.41833, best epoch:39, best blue:0.43051\n",
            "cur epoch:43, cur blue:0.42817, best epoch:39, best blue:0.43051\n",
            "cur epoch:44, cur blue:0.42475, best epoch:39, best blue:0.43051\n",
            "cur epoch:45, cur blue:0.42585, best epoch:39, best blue:0.43051\n",
            "cur epoch:46, cur blue:0.42120, best epoch:39, best blue:0.43051\n",
            "cur epoch:47, cur blue:0.41825, best epoch:39, best blue:0.43051\n",
            "cur epoch:48, cur blue:0.42095, best epoch:39, best blue:0.43051\n",
            "cur epoch:49, cur blue:0.42299, best epoch:39, best blue:0.43051\n",
            "cur epoch:50, cur blue:0.42859, best epoch:39, best blue:0.43051\n",
            "cur epoch:51, cur blue:0.42707, best epoch:39, best blue:0.43051\n",
            "cur epoch:52, cur blue:0.42716, best epoch:39, best blue:0.43051\n",
            "cur epoch:53, cur blue:0.42612, best epoch:39, best blue:0.43051\n",
            "cur epoch:54, cur blue:0.42695, best epoch:39, best blue:0.43051\n",
            "cur epoch:55, cur blue:0.43101, best epoch:55, best blue:0.43101\n",
            "cur epoch:56, cur blue:0.42772, best epoch:55, best blue:0.43101\n",
            "cur epoch:57, cur blue:0.42640, best epoch:55, best blue:0.43101\n",
            "cur epoch:58, cur blue:0.42379, best epoch:55, best blue:0.43101\n",
            "cur epoch:59, cur blue:0.41367, best epoch:55, best blue:0.43101\n",
            "cur epoch:60, cur blue:0.42401, best epoch:55, best blue:0.43101\n",
            "cur epoch:61, cur blue:0.42804, best epoch:55, best blue:0.43101\n",
            "cur epoch:62, cur blue:0.42717, best epoch:55, best blue:0.43101\n",
            "cur epoch:63, cur blue:0.42512, best epoch:55, best blue:0.43101\n",
            "cur epoch:64, cur blue:0.42393, best epoch:55, best blue:0.43101\n",
            "cur epoch:65, cur blue:0.43319, best epoch:65, best blue:0.43319\n",
            "cur epoch:66, cur blue:0.42659, best epoch:65, best blue:0.43319\n",
            "cur epoch:67, cur blue:0.42747, best epoch:65, best blue:0.43319\n",
            "cur epoch:68, cur blue:0.42025, best epoch:65, best blue:0.43319\n",
            "cur epoch:69, cur blue:0.43152, best epoch:65, best blue:0.43319\n",
            "cur epoch:70, cur blue:0.43372, best epoch:70, best blue:0.43372\n",
            "cur epoch:71, cur blue:0.43426, best epoch:71, best blue:0.43426\n",
            "cur epoch:72, cur blue:0.43535, best epoch:72, best blue:0.43535\n",
            "cur epoch:73, cur blue:0.42604, best epoch:72, best blue:0.43535\n",
            "cur epoch:74, cur blue:0.42098, best epoch:72, best blue:0.43535\n",
            "cur epoch:75, cur blue:0.43827, best epoch:75, best blue:0.43827\n",
            "cur epoch:76, cur blue:0.43973, best epoch:76, best blue:0.43973\n",
            "cur epoch:77, cur blue:0.43254, best epoch:76, best blue:0.43973\n",
            "cur epoch:78, cur blue:0.43611, best epoch:76, best blue:0.43973\n",
            "cur epoch:79, cur blue:0.43551, best epoch:76, best blue:0.43973\n",
            "cur epoch:80, cur blue:0.42445, best epoch:76, best blue:0.43973\n",
            "cur epoch:81, cur blue:0.43443, best epoch:76, best blue:0.43973\n",
            "cur epoch:82, cur blue:0.43290, best epoch:76, best blue:0.43973\n",
            "cur epoch:83, cur blue:0.42927, best epoch:76, best blue:0.43973\n",
            "cur epoch:84, cur blue:0.43476, best epoch:76, best blue:0.43973\n",
            "cur epoch:85, cur blue:0.42005, best epoch:76, best blue:0.43973\n",
            "cur epoch:86, cur blue:0.43138, best epoch:76, best blue:0.43973\n",
            "cur epoch:87, cur blue:0.43441, best epoch:76, best blue:0.43973\n",
            "cur epoch:88, cur blue:0.42541, best epoch:76, best blue:0.43973\n",
            "cur epoch:89, cur blue:0.43255, best epoch:76, best blue:0.43973\n",
            "cur epoch:90, cur blue:0.43105, best epoch:76, best blue:0.43973\n",
            "cur epoch:91, cur blue:0.43086, best epoch:76, best blue:0.43973\n",
            "cur epoch:92, cur blue:0.43278, best epoch:76, best blue:0.43973\n",
            "cur epoch:93, cur blue:0.43338, best epoch:76, best blue:0.43973\n",
            "cur epoch:94, cur blue:0.43295, best epoch:76, best blue:0.43973\n",
            "cur epoch:95, cur blue:0.43036, best epoch:76, best blue:0.43973\n",
            "cur epoch:96, cur blue:0.42617, best epoch:76, best blue:0.43973\n",
            "cur epoch:97, cur blue:0.43519, best epoch:76, best blue:0.43973\n",
            "cur epoch:98, cur blue:0.43789, best epoch:76, best blue:0.43973\n",
            "cur epoch:99, cur blue:0.43223, best epoch:76, best blue:0.43973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6IqAzu_FrLE",
        "outputId": "9a76511e-5489-40a0-97c6-2fcfaf45cc72"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "model_t4 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s4 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s4.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_4t.pth'))\n",
        "evaluate_matrics(model_s4,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.43973040628596327 Rouge: 0.4591312274684645 Meteor: 0.4262116411243918 PPL: 10.301880829149088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPP1pvZ9MVDt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCK2uLQaM3-s"
      },
      "source": [
        "Training with SD, T=5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ7fVcpiMd13",
        "outputId": "85fa6cc7-75c3-48d4-bcd1-c65b2432c305"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t5 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s5 = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "adam_optimizer = torch.optim.Adam(model_s5.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model_t5.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth'))\n",
        "\n",
        "T = 5\n",
        "criterionKD = SoftTarget(T)\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    \n",
        "    train_sd(train_loader, model_t5, model_s5, criterion, criterionKD, transformer_optimizer, epoch)\n",
        "    blue_score = valid (test_loader, model_s5)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_s5.state_dict(), '/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_5t.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00001, best epoch:0, best blue:0.00001\n",
            "cur epoch:1, cur blue:0.05770, best epoch:1, best blue:0.05770\n",
            "cur epoch:2, cur blue:0.14118, best epoch:2, best blue:0.14118\n",
            "cur epoch:3, cur blue:0.14310, best epoch:3, best blue:0.14310\n",
            "cur epoch:4, cur blue:0.17985, best epoch:4, best blue:0.17985\n",
            "cur epoch:5, cur blue:0.22287, best epoch:5, best blue:0.22287\n",
            "cur epoch:6, cur blue:0.26217, best epoch:6, best blue:0.26217\n",
            "cur epoch:7, cur blue:0.27164, best epoch:7, best blue:0.27164\n",
            "cur epoch:8, cur blue:0.27115, best epoch:7, best blue:0.27164\n",
            "cur epoch:9, cur blue:0.31345, best epoch:9, best blue:0.31345\n",
            "cur epoch:10, cur blue:0.33722, best epoch:10, best blue:0.33722\n",
            "cur epoch:11, cur blue:0.32965, best epoch:10, best blue:0.33722\n",
            "cur epoch:12, cur blue:0.33971, best epoch:12, best blue:0.33971\n",
            "cur epoch:13, cur blue:0.33675, best epoch:12, best blue:0.33971\n",
            "cur epoch:14, cur blue:0.34395, best epoch:14, best blue:0.34395\n",
            "cur epoch:15, cur blue:0.31852, best epoch:14, best blue:0.34395\n",
            "cur epoch:16, cur blue:0.31757, best epoch:14, best blue:0.34395\n",
            "cur epoch:17, cur blue:0.34752, best epoch:17, best blue:0.34752\n",
            "cur epoch:18, cur blue:0.36377, best epoch:18, best blue:0.36377\n",
            "cur epoch:19, cur blue:0.38177, best epoch:19, best blue:0.38177\n",
            "cur epoch:20, cur blue:0.37320, best epoch:19, best blue:0.38177\n",
            "cur epoch:21, cur blue:0.37966, best epoch:19, best blue:0.38177\n",
            "cur epoch:22, cur blue:0.38734, best epoch:22, best blue:0.38734\n",
            "cur epoch:23, cur blue:0.39208, best epoch:23, best blue:0.39208\n",
            "cur epoch:24, cur blue:0.38845, best epoch:23, best blue:0.39208\n",
            "cur epoch:25, cur blue:0.38989, best epoch:23, best blue:0.39208\n",
            "cur epoch:26, cur blue:0.40492, best epoch:26, best blue:0.40492\n",
            "cur epoch:27, cur blue:0.40706, best epoch:27, best blue:0.40706\n",
            "cur epoch:28, cur blue:0.40353, best epoch:27, best blue:0.40706\n",
            "cur epoch:29, cur blue:0.40451, best epoch:27, best blue:0.40706\n",
            "cur epoch:30, cur blue:0.41166, best epoch:30, best blue:0.41166\n",
            "cur epoch:31, cur blue:0.40758, best epoch:30, best blue:0.41166\n",
            "cur epoch:32, cur blue:0.41819, best epoch:32, best blue:0.41819\n",
            "cur epoch:33, cur blue:0.41042, best epoch:32, best blue:0.41819\n",
            "cur epoch:34, cur blue:0.40641, best epoch:32, best blue:0.41819\n",
            "cur epoch:35, cur blue:0.41785, best epoch:32, best blue:0.41819\n",
            "cur epoch:36, cur blue:0.41896, best epoch:36, best blue:0.41896\n",
            "cur epoch:37, cur blue:0.42138, best epoch:37, best blue:0.42138\n",
            "cur epoch:38, cur blue:0.42328, best epoch:38, best blue:0.42328\n",
            "cur epoch:39, cur blue:0.41330, best epoch:38, best blue:0.42328\n",
            "cur epoch:40, cur blue:0.41791, best epoch:38, best blue:0.42328\n",
            "cur epoch:41, cur blue:0.41412, best epoch:38, best blue:0.42328\n",
            "cur epoch:42, cur blue:0.41768, best epoch:38, best blue:0.42328\n",
            "cur epoch:43, cur blue:0.41959, best epoch:38, best blue:0.42328\n",
            "cur epoch:44, cur blue:0.43084, best epoch:44, best blue:0.43084\n",
            "cur epoch:45, cur blue:0.42131, best epoch:44, best blue:0.43084\n",
            "cur epoch:46, cur blue:0.41968, best epoch:44, best blue:0.43084\n",
            "cur epoch:47, cur blue:0.42119, best epoch:44, best blue:0.43084\n",
            "cur epoch:48, cur blue:0.42068, best epoch:44, best blue:0.43084\n",
            "cur epoch:49, cur blue:0.41446, best epoch:44, best blue:0.43084\n",
            "cur epoch:50, cur blue:0.42842, best epoch:44, best blue:0.43084\n",
            "cur epoch:51, cur blue:0.41880, best epoch:44, best blue:0.43084\n",
            "cur epoch:52, cur blue:0.42089, best epoch:44, best blue:0.43084\n",
            "cur epoch:53, cur blue:0.42412, best epoch:44, best blue:0.43084\n",
            "cur epoch:54, cur blue:0.42766, best epoch:44, best blue:0.43084\n",
            "cur epoch:55, cur blue:0.43245, best epoch:55, best blue:0.43245\n",
            "cur epoch:56, cur blue:0.42112, best epoch:55, best blue:0.43245\n",
            "cur epoch:57, cur blue:0.43221, best epoch:55, best blue:0.43245\n",
            "cur epoch:58, cur blue:0.42582, best epoch:55, best blue:0.43245\n",
            "cur epoch:59, cur blue:0.42269, best epoch:55, best blue:0.43245\n",
            "cur epoch:60, cur blue:0.42543, best epoch:55, best blue:0.43245\n",
            "cur epoch:61, cur blue:0.44316, best epoch:61, best blue:0.44316\n",
            "cur epoch:62, cur blue:0.42035, best epoch:61, best blue:0.44316\n",
            "cur epoch:63, cur blue:0.43353, best epoch:61, best blue:0.44316\n",
            "cur epoch:64, cur blue:0.43053, best epoch:61, best blue:0.44316\n",
            "cur epoch:65, cur blue:0.43009, best epoch:61, best blue:0.44316\n",
            "cur epoch:66, cur blue:0.43101, best epoch:61, best blue:0.44316\n",
            "cur epoch:67, cur blue:0.43951, best epoch:61, best blue:0.44316\n",
            "cur epoch:68, cur blue:0.42739, best epoch:61, best blue:0.44316\n",
            "cur epoch:69, cur blue:0.42896, best epoch:61, best blue:0.44316\n",
            "cur epoch:70, cur blue:0.43183, best epoch:61, best blue:0.44316\n",
            "cur epoch:71, cur blue:0.43214, best epoch:61, best blue:0.44316\n",
            "cur epoch:72, cur blue:0.42579, best epoch:61, best blue:0.44316\n",
            "cur epoch:73, cur blue:0.42442, best epoch:61, best blue:0.44316\n",
            "cur epoch:74, cur blue:0.43228, best epoch:61, best blue:0.44316\n",
            "cur epoch:75, cur blue:0.42992, best epoch:61, best blue:0.44316\n",
            "cur epoch:76, cur blue:0.43518, best epoch:61, best blue:0.44316\n",
            "cur epoch:77, cur blue:0.43306, best epoch:61, best blue:0.44316\n",
            "cur epoch:78, cur blue:0.43708, best epoch:61, best blue:0.44316\n",
            "cur epoch:79, cur blue:0.43428, best epoch:61, best blue:0.44316\n",
            "cur epoch:80, cur blue:0.43250, best epoch:61, best blue:0.44316\n",
            "cur epoch:81, cur blue:0.43183, best epoch:61, best blue:0.44316\n",
            "cur epoch:82, cur blue:0.43528, best epoch:61, best blue:0.44316\n",
            "cur epoch:83, cur blue:0.42790, best epoch:61, best blue:0.44316\n",
            "cur epoch:84, cur blue:0.42344, best epoch:61, best blue:0.44316\n",
            "cur epoch:85, cur blue:0.43006, best epoch:61, best blue:0.44316\n",
            "cur epoch:86, cur blue:0.42852, best epoch:61, best blue:0.44316\n",
            "cur epoch:87, cur blue:0.43719, best epoch:61, best blue:0.44316\n",
            "cur epoch:88, cur blue:0.43168, best epoch:61, best blue:0.44316\n",
            "cur epoch:89, cur blue:0.42930, best epoch:61, best blue:0.44316\n",
            "cur epoch:90, cur blue:0.42447, best epoch:61, best blue:0.44316\n",
            "cur epoch:91, cur blue:0.42873, best epoch:61, best blue:0.44316\n",
            "cur epoch:92, cur blue:0.42582, best epoch:61, best blue:0.44316\n",
            "cur epoch:93, cur blue:0.43487, best epoch:61, best blue:0.44316\n",
            "cur epoch:94, cur blue:0.43006, best epoch:61, best blue:0.44316\n",
            "cur epoch:95, cur blue:0.42897, best epoch:61, best blue:0.44316\n",
            "cur epoch:96, cur blue:0.43378, best epoch:61, best blue:0.44316\n",
            "cur epoch:97, cur blue:0.42611, best epoch:61, best blue:0.44316\n",
            "cur epoch:98, cur blue:0.42743, best epoch:61, best blue:0.44316\n",
            "cur epoch:99, cur blue:0.43103, best epoch:61, best blue:0.44316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfX68puUMeS-",
        "outputId": "526b4ee7-bd5d-4380-9dbf-78fb1903c98b"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "model_s5.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_chatbot_models_sd_5t.pth'))\n",
        "evaluate_matrics(model_s5,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.44316121415895837 Rouge: 0.4596136337718651 Meteor: 0.4286404674318712 PPL: 9.125769864393439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbHfE4aWMv07"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5L4NMbxe9l5",
        "outputId": "848df3ca-9d76-443c-e85f-5508e63c5a72"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    train(train_loader, model, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        \n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/calibration_project/medical_dialogue_system/best_models_baseline.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00567, best epoch:0, best blue:0.00567\n",
            "cur epoch:1, cur blue:0.13392, best epoch:1, best blue:0.13392\n",
            "cur epoch:2, cur blue:0.16344, best epoch:2, best blue:0.16344\n",
            "cur epoch:3, cur blue:0.18116, best epoch:3, best blue:0.18116\n",
            "cur epoch:4, cur blue:0.20916, best epoch:4, best blue:0.20916\n",
            "cur epoch:5, cur blue:0.23907, best epoch:5, best blue:0.23907\n",
            "cur epoch:6, cur blue:0.28745, best epoch:6, best blue:0.28745\n",
            "cur epoch:7, cur blue:0.31096, best epoch:7, best blue:0.31096\n",
            "cur epoch:8, cur blue:0.33110, best epoch:8, best blue:0.33110\n",
            "cur epoch:9, cur blue:0.33065, best epoch:8, best blue:0.33110\n",
            "cur epoch:10, cur blue:0.33944, best epoch:10, best blue:0.33944\n",
            "cur epoch:11, cur blue:0.34200, best epoch:11, best blue:0.34200\n",
            "cur epoch:12, cur blue:0.34763, best epoch:12, best blue:0.34763\n",
            "cur epoch:13, cur blue:0.34833, best epoch:13, best blue:0.34833\n",
            "cur epoch:14, cur blue:0.34357, best epoch:13, best blue:0.34833\n",
            "cur epoch:15, cur blue:0.32893, best epoch:13, best blue:0.34833\n",
            "cur epoch:16, cur blue:0.34632, best epoch:13, best blue:0.34833\n",
            "cur epoch:17, cur blue:0.34920, best epoch:17, best blue:0.34920\n",
            "cur epoch:18, cur blue:0.35441, best epoch:18, best blue:0.35441\n",
            "cur epoch:19, cur blue:0.37666, best epoch:19, best blue:0.37666\n",
            "cur epoch:20, cur blue:0.34513, best epoch:19, best blue:0.37666\n",
            "cur epoch:21, cur blue:0.38160, best epoch:21, best blue:0.38160\n",
            "cur epoch:22, cur blue:0.37871, best epoch:21, best blue:0.38160\n",
            "cur epoch:23, cur blue:0.39120, best epoch:23, best blue:0.39120\n",
            "cur epoch:24, cur blue:0.38543, best epoch:23, best blue:0.39120\n",
            "cur epoch:25, cur blue:0.38628, best epoch:23, best blue:0.39120\n",
            "cur epoch:26, cur blue:0.36373, best epoch:23, best blue:0.39120\n",
            "cur epoch:27, cur blue:0.39635, best epoch:27, best blue:0.39635\n",
            "cur epoch:28, cur blue:0.41370, best epoch:28, best blue:0.41370\n",
            "cur epoch:29, cur blue:0.40595, best epoch:28, best blue:0.41370\n",
            "cur epoch:30, cur blue:0.39411, best epoch:28, best blue:0.41370\n",
            "cur epoch:31, cur blue:0.39011, best epoch:28, best blue:0.41370\n",
            "cur epoch:32, cur blue:0.39413, best epoch:28, best blue:0.41370\n",
            "cur epoch:33, cur blue:0.40272, best epoch:28, best blue:0.41370\n",
            "cur epoch:34, cur blue:0.40783, best epoch:28, best blue:0.41370\n",
            "cur epoch:35, cur blue:0.40821, best epoch:28, best blue:0.41370\n",
            "cur epoch:36, cur blue:0.40313, best epoch:28, best blue:0.41370\n",
            "cur epoch:37, cur blue:0.41568, best epoch:37, best blue:0.41568\n",
            "cur epoch:38, cur blue:0.40382, best epoch:37, best blue:0.41568\n",
            "cur epoch:39, cur blue:0.42110, best epoch:39, best blue:0.42110\n",
            "cur epoch:40, cur blue:0.41603, best epoch:39, best blue:0.42110\n",
            "cur epoch:41, cur blue:0.41038, best epoch:39, best blue:0.42110\n",
            "cur epoch:42, cur blue:0.40741, best epoch:39, best blue:0.42110\n",
            "cur epoch:43, cur blue:0.41109, best epoch:39, best blue:0.42110\n",
            "cur epoch:44, cur blue:0.41534, best epoch:39, best blue:0.42110\n",
            "cur epoch:45, cur blue:0.41907, best epoch:39, best blue:0.42110\n",
            "cur epoch:46, cur blue:0.42040, best epoch:39, best blue:0.42110\n",
            "cur epoch:47, cur blue:0.41214, best epoch:39, best blue:0.42110\n",
            "cur epoch:48, cur blue:0.41098, best epoch:39, best blue:0.42110\n",
            "cur epoch:49, cur blue:0.41245, best epoch:39, best blue:0.42110\n",
            "cur epoch:50, cur blue:0.42686, best epoch:50, best blue:0.42686\n",
            "cur epoch:51, cur blue:0.42260, best epoch:50, best blue:0.42686\n",
            "cur epoch:52, cur blue:0.41236, best epoch:50, best blue:0.42686\n",
            "cur epoch:53, cur blue:0.43047, best epoch:53, best blue:0.43047\n",
            "cur epoch:54, cur blue:0.41813, best epoch:53, best blue:0.43047\n",
            "cur epoch:55, cur blue:0.41565, best epoch:53, best blue:0.43047\n",
            "cur epoch:56, cur blue:0.42550, best epoch:53, best blue:0.43047\n",
            "cur epoch:57, cur blue:0.42707, best epoch:53, best blue:0.43047\n",
            "cur epoch:58, cur blue:0.43077, best epoch:58, best blue:0.43077\n",
            "cur epoch:59, cur blue:0.41695, best epoch:58, best blue:0.43077\n",
            "cur epoch:60, cur blue:0.41991, best epoch:58, best blue:0.43077\n",
            "cur epoch:61, cur blue:0.42801, best epoch:58, best blue:0.43077\n",
            "cur epoch:62, cur blue:0.41886, best epoch:58, best blue:0.43077\n",
            "cur epoch:63, cur blue:0.42404, best epoch:58, best blue:0.43077\n",
            "cur epoch:64, cur blue:0.42353, best epoch:58, best blue:0.43077\n",
            "cur epoch:65, cur blue:0.42516, best epoch:58, best blue:0.43077\n",
            "cur epoch:66, cur blue:0.42410, best epoch:58, best blue:0.43077\n",
            "cur epoch:67, cur blue:0.42836, best epoch:58, best blue:0.43077\n",
            "cur epoch:68, cur blue:0.42027, best epoch:58, best blue:0.43077\n",
            "cur epoch:69, cur blue:0.42806, best epoch:58, best blue:0.43077\n",
            "cur epoch:70, cur blue:0.41802, best epoch:58, best blue:0.43077\n",
            "cur epoch:71, cur blue:0.42651, best epoch:58, best blue:0.43077\n",
            "cur epoch:72, cur blue:0.42374, best epoch:58, best blue:0.43077\n",
            "cur epoch:73, cur blue:0.42891, best epoch:58, best blue:0.43077\n",
            "cur epoch:74, cur blue:0.42486, best epoch:58, best blue:0.43077\n",
            "cur epoch:75, cur blue:0.42939, best epoch:58, best blue:0.43077\n",
            "cur epoch:76, cur blue:0.42736, best epoch:58, best blue:0.43077\n",
            "cur epoch:77, cur blue:0.43096, best epoch:77, best blue:0.43096\n",
            "cur epoch:78, cur blue:0.42647, best epoch:77, best blue:0.43096\n",
            "cur epoch:79, cur blue:0.42112, best epoch:77, best blue:0.43096\n",
            "cur epoch:80, cur blue:0.42815, best epoch:77, best blue:0.43096\n",
            "cur epoch:81, cur blue:0.42769, best epoch:77, best blue:0.43096\n",
            "cur epoch:82, cur blue:0.41759, best epoch:77, best blue:0.43096\n",
            "cur epoch:83, cur blue:0.42303, best epoch:77, best blue:0.43096\n",
            "cur epoch:84, cur blue:0.43390, best epoch:84, best blue:0.43390\n",
            "cur epoch:85, cur blue:0.41752, best epoch:84, best blue:0.43390\n",
            "cur epoch:86, cur blue:0.42701, best epoch:84, best blue:0.43390\n",
            "cur epoch:87, cur blue:0.42617, best epoch:84, best blue:0.43390\n",
            "cur epoch:88, cur blue:0.41856, best epoch:84, best blue:0.43390\n",
            "cur epoch:89, cur blue:0.42067, best epoch:84, best blue:0.43390\n",
            "cur epoch:90, cur blue:0.41905, best epoch:84, best blue:0.43390\n",
            "cur epoch:91, cur blue:0.43254, best epoch:84, best blue:0.43390\n",
            "cur epoch:92, cur blue:0.42691, best epoch:84, best blue:0.43390\n",
            "cur epoch:93, cur blue:0.43236, best epoch:84, best blue:0.43390\n",
            "cur epoch:94, cur blue:0.42943, best epoch:84, best blue:0.43390\n",
            "cur epoch:95, cur blue:0.42919, best epoch:84, best blue:0.43390\n",
            "cur epoch:96, cur blue:0.42223, best epoch:84, best blue:0.43390\n",
            "cur epoch:97, cur blue:0.43132, best epoch:84, best blue:0.43390\n",
            "cur epoch:98, cur blue:0.42828, best epoch:84, best blue:0.43390\n",
            "cur epoch:99, cur blue:0.42567, best epoch:84, best blue:0.43390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKdCPy01ClQT",
        "outputId": "9c95fe7e-8535-4008-81d2-133cd6f37bc1"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "evaluate_matrics(model,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.4256746608835476 Rouge: 0.45004191513969954 Meteor: 0.4090250663724253 PPL: 8.381151839452423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR3YM48htJ9K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}