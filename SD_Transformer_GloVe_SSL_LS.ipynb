{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SD_Transformer_GloVe_SSL_LS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/Doti/blob/main/SD_Transformer_GloVe_SSL_LS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7-Yal7-o0vN",
        "outputId": "5ba09c04-3adc-477e-95b5-58481ff09588"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "#system\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import json\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "  #random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaC-MomvBDAv",
        "outputId": "48ad387f-49be-460c-edfd-b182bc61654a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJkwP6eaB589",
        "outputId": "746fe3f1-b424-4ea4-a1b0-f9738a553e9e"
      },
      "source": [
        "!pip install \"nltk==3.4.5\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.6/dist-packages (3.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFHqt1eepjjr"
      },
      "source": [
        "max_length = 35\n",
        "#train_dir = \"/content/drive/MyDrive/chatbot/combined_qa_train_ID.csv\"\n",
        "#test_dir = \"/content/drive/MyDrive/chatbot/combined_qa_test_50_ID.csv\"\n",
        "train_dir = \"/content/drive/MyDrive/chatbot/combined_qa_train_ID.csv\"\n",
        "test_dir = \"/content/drive/MyDrive/chatbot/combined_qa_test_200_ID.csv\"\n",
        "batch_size = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence.lower()\n",
        "\n",
        "def prepare_csv(train,test):\n",
        "    # idx = np.arange(df_train.shape[0])    \n",
        "    # np.random.shuffle(idx)\n",
        "    # val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    train_temp = train[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_train.csv', index=True)\n",
        "    \n",
        "    test_temp = test[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_val.csv', index=True) \n",
        "    return  train_temp,  test_temp\n",
        "\n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size, device=None): \n",
        "    train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    test =  pd.read_csv(test_dir,error_bad_lines=False)\n",
        "    train['Question'] = train['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    train['Answer'] = train['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Question'] = test['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Answer'] = test['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    train_temp,  test_temp = prepare_csv(train,test)\n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "\n",
        "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,batch_first=True, \n",
        "                      fix_length=fix_length)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)  \n",
        "    train_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_train.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "    test_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_val.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "\n",
        "    TEXT.build_vocab(train_temps,test_temps, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps, test_temps)\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    print(\"vocab_size_and_ntokens:\",vocab_size,ntokens)\n",
        "    train_loader = get_iterator(train_temps, batch_size=batch_size, \n",
        "                                train=True, shuffle=True,\n",
        "                                repeat=False,device=None)\n",
        "    test_loader = get_iterator(test_temps, batch_size=batch_size, \n",
        "                            train=False, shuffle=False,\n",
        "                            repeat=False, device=None)\n",
        "    print('Train samples:%d'%(len(train_temps)), 'Valid samples:%d'%(len(test_temps)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "            'Valid minibatch nb:%d'%(len(test_loader)))\n",
        "    return vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrQV-PKdqHOO",
        "outputId": "4ba8f61e-ee8f-4e75-e5ea-a5fd4f2fe178"
      },
      "source": [
        "vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT = get_dataset(fix_length=max_length,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                           \n",
            "100%|█████████▉| 399264/400000 [00:39<00:00, 10870.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab_size_and_ntokens: 1938 1938\n",
            "Train samples:1001 Valid samples:200 Train minibatch nb:251 Valid minibatch nb:50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7PgYwYcqNP1"
      },
      "source": [
        "def create_masks(question, reply_input,reply_target):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "\n",
        "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
        "    # if question.size(0) != bptt:\n",
        "    #     question_mask = create_masks(question.size(0)).to(device)\n",
        "    # if reply_input.size(0) != bptt:\n",
        "    #     reply_input_mask = create_masks(reply_input.size(0)).to(device)\n",
        "    \n",
        "    return question_mask, reply_input_mask,reply_target_mask\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaD0bzWAsTme"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        #print(\"embedding\",embedding.size(),encoded_words.size())\n",
        "        #print(\"pe\",self.pe.size())\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)   \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed_dec(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        #out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        out = self.logit(decoded)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qbLMn34sYmS"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "    #for i, (question, reply) in enumerate(train_loader):     \n",
        "        #samples = question.shape[0]\n",
        "        # Move to device\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        # Prepare Target Data\n",
        "        #print(\"pair\",pair, type(pair),len(pair))\n",
        "        #print(\"reply\",type(reply))\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask,reply_target_mask = create_masks(question, reply_input,reply_target)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        #reply_target = reply_target.reshape(-1)\n",
        "        loss = criterion(out, reply_target,reply_target_mask)\n",
        "        #loss = criterion(out, reply_target)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        #sum_loss += loss.item() * samples\n",
        "        #count += samples\n",
        "        #break\n",
        "        # if i % 100 == 0:\n",
        "        #     print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    all_blue = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "    #for i, (question, reply) in enumerate(test_loader):\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask,reply_target_mask = create_masks(question, reply_input,reply_target)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            # BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            # BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            # BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            all_blue.append(BLEU_1)\n",
        "    #print(\"BLEU_score:\",np.mean(all_blue))\n",
        "    return np.mean(all_blue)\n",
        "\n",
        "def evaluate(transformer, question, question_mask, max_len):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<sos>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    next_word = -22\n",
        "    while next_word != word_map['<eos>']:\n",
        "    #for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<eos>'] or words.shape[1]==(max_len+1):\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<sos>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "\n",
        "    return sentence\n",
        "\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask,reply_target_mask = create_masks(question, reply_input,reply_target)\n",
        "        #question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        #loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_dUpT_rBPqO"
      },
      "source": [
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            #print(outputs.shape)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            # target_loss = targets.reshape(-1)\n",
        "            # loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            # test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyNgTQMRse52",
        "outputId": "4db76a68-f187-48de-b81a-b8ebdf5fbd8a"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = LossWithLS(ntokens, 0.1)\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(30):\n",
        "    train(train_loader, model, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object_LS0.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_LS0.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.01450, best epoch:0, best blue:0.01450\n",
            "cur epoch:1, cur blue:0.12230, best epoch:1, best blue:0.12230\n",
            "cur epoch:2, cur blue:0.15650, best epoch:2, best blue:0.15650\n",
            "cur epoch:3, cur blue:0.16440, best epoch:3, best blue:0.16440\n",
            "cur epoch:4, cur blue:0.22024, best epoch:4, best blue:0.22024\n",
            "cur epoch:5, cur blue:0.25586, best epoch:5, best blue:0.25586\n",
            "cur epoch:6, cur blue:0.27678, best epoch:6, best blue:0.27678\n",
            "cur epoch:7, cur blue:0.32585, best epoch:7, best blue:0.32585\n",
            "cur epoch:8, cur blue:0.32860, best epoch:8, best blue:0.32860\n",
            "cur epoch:9, cur blue:0.34322, best epoch:9, best blue:0.34322\n",
            "cur epoch:10, cur blue:0.34119, best epoch:9, best blue:0.34322\n",
            "cur epoch:11, cur blue:0.34936, best epoch:11, best blue:0.34936\n",
            "cur epoch:12, cur blue:0.33641, best epoch:11, best blue:0.34936\n",
            "cur epoch:13, cur blue:0.34630, best epoch:11, best blue:0.34936\n",
            "cur epoch:14, cur blue:0.35724, best epoch:14, best blue:0.35724\n",
            "cur epoch:15, cur blue:0.33781, best epoch:14, best blue:0.35724\n",
            "cur epoch:16, cur blue:0.34340, best epoch:14, best blue:0.35724\n",
            "cur epoch:17, cur blue:0.34281, best epoch:14, best blue:0.35724\n",
            "cur epoch:18, cur blue:0.35999, best epoch:18, best blue:0.35999\n",
            "cur epoch:19, cur blue:0.37131, best epoch:19, best blue:0.37131\n",
            "cur epoch:20, cur blue:0.39917, best epoch:20, best blue:0.39917\n",
            "cur epoch:21, cur blue:0.38135, best epoch:20, best blue:0.39917\n",
            "cur epoch:22, cur blue:0.38489, best epoch:20, best blue:0.39917\n",
            "cur epoch:23, cur blue:0.39951, best epoch:23, best blue:0.39951\n",
            "cur epoch:24, cur blue:0.41481, best epoch:24, best blue:0.41481\n",
            "cur epoch:25, cur blue:0.42428, best epoch:25, best blue:0.42428\n",
            "cur epoch:26, cur blue:0.42029, best epoch:25, best blue:0.42428\n",
            "cur epoch:27, cur blue:0.41290, best epoch:25, best blue:0.42428\n",
            "cur epoch:28, cur blue:0.41808, best epoch:25, best blue:0.42428\n",
            "cur epoch:29, cur blue:0.41677, best epoch:25, best blue:0.42428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEqZIlHY1lN0"
      },
      "source": [
        "def create_masks(question, reply_input):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        #print(\"embedding\",embedding.size(),encoded_words.size())\n",
        "        #print(\"pe\",self.pe.size())\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)   \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed_dec(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def train(train_loader, transformer, criterion, epoch):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "    #for i, (question, reply) in enumerate(train_loader):     \n",
        "        #samples = question.shape[0]\n",
        "        # Move to device\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        # Prepare Target Data\n",
        "        #print(\"pair\",pair, type(pair),len(pair))\n",
        "        #print(\"reply\",type(reply))\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target)\n",
        "        #loss = criterion(out, reply_target)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        #sum_loss += loss.item() * samples\n",
        "        #count += samples\n",
        "        #break\n",
        "        # if i % 100 == 0:\n",
        "        #     print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    all_blue = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "    #for i, (question, reply) in enumerate(test_loader):\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            # BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            # BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            # BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            all_blue.append(BLEU_1)\n",
        "    #print(\"BLEU_score:\",np.mean(all_blue))\n",
        "    return np.mean(all_blue)\n",
        "\n",
        "def evaluate(transformer, question, question_mask, max_len):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<sos>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    next_word = -22\n",
        "    while next_word != word_map['<eos>']:\n",
        "    #for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<eos>'] or words.shape[1]==(max_len+1):\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<sos>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "\n",
        "    return sentence\n",
        "\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))\n",
        "\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            #print(outputs.shape)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            # target_loss = targets.reshape(-1)\n",
        "            # loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            # test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfWg-zh7ssgL",
        "outputId": "d27d7e24-c3be-4e92-bc45-34cb1b658c24"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_LS0.pth'))\n",
        "model = model.to(device)\n",
        "best_blue_SSL = 0\n",
        "best_epoch_SSL = 0\n",
        "for epoch_SSL in range(30):\n",
        "    train(train_loader, model, criterion, epoch_SSL)\n",
        "    blue_score_SSL = valid (test_loader,model)\n",
        "    if blue_score_SSL > best_blue_SSL:\n",
        "        best_blue_SSL = blue_score_SSL\n",
        "        best_epoch_SSL = epoch_SSL\n",
        "        state = {'epoch': epoch_SSL, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object_SSL_LS1.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL_LS1.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch_SSL,blue_score_SSL, best_epoch_SSL, best_blue_SSL))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.43751, best epoch:0, best blue:0.43751\n",
            "cur epoch:1, cur blue:0.44595, best epoch:1, best blue:0.44595\n",
            "cur epoch:2, cur blue:0.45969, best epoch:2, best blue:0.45969\n",
            "cur epoch:3, cur blue:0.44988, best epoch:2, best blue:0.45969\n",
            "cur epoch:4, cur blue:0.45946, best epoch:2, best blue:0.45969\n",
            "cur epoch:5, cur blue:0.44519, best epoch:2, best blue:0.45969\n",
            "cur epoch:6, cur blue:0.44040, best epoch:2, best blue:0.45969\n",
            "cur epoch:7, cur blue:0.44584, best epoch:2, best blue:0.45969\n",
            "cur epoch:8, cur blue:0.43474, best epoch:2, best blue:0.45969\n",
            "cur epoch:9, cur blue:0.43887, best epoch:2, best blue:0.45969\n",
            "cur epoch:10, cur blue:0.44328, best epoch:2, best blue:0.45969\n",
            "cur epoch:11, cur blue:0.42181, best epoch:2, best blue:0.45969\n",
            "cur epoch:12, cur blue:0.39003, best epoch:2, best blue:0.45969\n",
            "cur epoch:13, cur blue:0.38936, best epoch:2, best blue:0.45969\n",
            "cur epoch:14, cur blue:0.38285, best epoch:2, best blue:0.45969\n",
            "cur epoch:15, cur blue:0.37933, best epoch:2, best blue:0.45969\n",
            "cur epoch:16, cur blue:0.36779, best epoch:2, best blue:0.45969\n",
            "cur epoch:17, cur blue:0.40334, best epoch:2, best blue:0.45969\n",
            "cur epoch:18, cur blue:0.36786, best epoch:2, best blue:0.45969\n",
            "cur epoch:19, cur blue:0.40484, best epoch:2, best blue:0.45969\n",
            "cur epoch:20, cur blue:0.39491, best epoch:2, best blue:0.45969\n",
            "cur epoch:21, cur blue:0.39847, best epoch:2, best blue:0.45969\n",
            "cur epoch:22, cur blue:0.39464, best epoch:2, best blue:0.45969\n",
            "cur epoch:23, cur blue:0.40112, best epoch:2, best blue:0.45969\n",
            "cur epoch:24, cur blue:0.39937, best epoch:2, best blue:0.45969\n",
            "cur epoch:25, cur blue:0.40582, best epoch:2, best blue:0.45969\n",
            "cur epoch:26, cur blue:0.41130, best epoch:2, best blue:0.45969\n",
            "cur epoch:27, cur blue:0.40733, best epoch:2, best blue:0.45969\n",
            "cur epoch:28, cur blue:0.41201, best epoch:2, best blue:0.45969\n",
            "cur epoch:29, cur blue:0.39324, best epoch:2, best blue:0.45969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ibr7Wety83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43c9ea4-192a-44fb-ce80-8727554478bd"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL_LS1.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.4596904676020052 Precision: 0.5527765208596511 Recall: 0.5183230765127406 F1_Score: 0.5295769509258413 Meteor: 0.44655928379976473 PPL: 5.381701360498236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3590400815010071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMPDaCdIIICe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNyV9HC440Ca"
      },
      "source": [
        "## TS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5dC-D6G41R_",
        "outputId": "3a9fc0e9-67f5-46fe-d31d-4be0c0ea5145"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, inputs, input_mask, targets, targets_mask):\n",
        "        logits = self.model(inputs, input_mask, targets, targets_mask)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size())\n",
        "        return logits / temperature\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        self.cuda()\n",
        "        ece_criterion = _ECELoss().cuda()\n",
        "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for i, pair in enumerate(valid_loader):\n",
        "    \n",
        "                input = pair.Question.cuda()\n",
        "                label = pair.Answer.cuda()\n",
        "            #for input, label in valid_loader:\n",
        "                input = input.cuda()\n",
        "                label = label.cuda()\n",
        "                label = label[:, 1:]\n",
        "                input_mask, label_mask = create_masks(input, label)\n",
        "                logits = self.model(input, input_mask, label, label_mask)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).cuda()\n",
        "            labels = torch.cat(labels_list).cuda()\n",
        "            \n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        init_temp = self.temperature.clone()\n",
        "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "        def eval():\n",
        "            labels_loss = labels.reshape(-1)\n",
        "            loss = nll_criterion(self.temperature_scale(logits.view(-1, ntokens)), labels_loss)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(eval)\n",
        "\n",
        "        # CalculateECE after temperature scaling\n",
        "        labels_loss = labels.reshape(-1)\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits.view(-1,ntokens )), labels_loss).item()\n",
        "        print('Initial temperature: %.3f, Optimal temperature: %.3f' % (init_temp, self.temperature.item()))\n",
        "        return self\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            target_loss = targets.reshape(-1)\n",
        "            loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL_LS1.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('Before TS- bleu:%.3f, bef ece:%.5f'%(bleu,temperature_ece))\n",
        "\n",
        "\n",
        "model_ts = ModelWithTemperature(model)\n",
        "model_ts.set_temperature(test_loader)\n",
        "bleu, logits_all, labels_all = evaluation(model_ts, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('After TS- bleu:%.3f,aft ece:%.5f'%(bleu,temperature_ece))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before TS- bleu:0.428, bef ece:0.35904\n",
            "Initial temperature: 1.500, Optimal temperature: 2.196\n",
            "After TS- bleu:0.428,aft ece:0.17882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzuX94MD9san",
        "outputId": "732e885f-f8ff-4f9a-a24a-fafff0564334"
      },
      "source": [
        "model_ts.temperature = 4.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([2.5142], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtwbAT604_Bd",
        "outputId": "8b71769f-c663-4344-f673-c4767f2d51f1"
      },
      "source": [
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(50):\n",
        "    train(train_loader, model_ts, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model_ts)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model_ts, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object_ULMFIT_SSL_LS_TS1.pth.tar')\n",
        "        torch.save(model_ts.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_ULMFIT_SSL_LS_TS1.pth')\n",
        "    print('cur epoch:%d, cur blue:%.4f, best epoch:%d, best blue:%.4f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.4183, best epoch:0, best blue:0.4183\n",
            "cur epoch:1, cur blue:0.4304, best epoch:1, best blue:0.4304\n",
            "cur epoch:2, cur blue:0.4315, best epoch:2, best blue:0.4315\n",
            "cur epoch:3, cur blue:0.4266, best epoch:2, best blue:0.4315\n",
            "cur epoch:4, cur blue:0.4338, best epoch:4, best blue:0.4338\n",
            "cur epoch:5, cur blue:0.4319, best epoch:4, best blue:0.4338\n",
            "cur epoch:6, cur blue:0.4219, best epoch:4, best blue:0.4338\n",
            "cur epoch:7, cur blue:0.4314, best epoch:4, best blue:0.4338\n",
            "cur epoch:8, cur blue:0.4253, best epoch:4, best blue:0.4338\n",
            "cur epoch:9, cur blue:0.4279, best epoch:4, best blue:0.4338\n",
            "cur epoch:10, cur blue:0.4146, best epoch:4, best blue:0.4338\n",
            "cur epoch:11, cur blue:0.4003, best epoch:4, best blue:0.4338\n",
            "cur epoch:12, cur blue:0.4089, best epoch:4, best blue:0.4338\n",
            "cur epoch:13, cur blue:0.4093, best epoch:4, best blue:0.4338\n",
            "cur epoch:14, cur blue:0.4193, best epoch:4, best blue:0.4338\n",
            "cur epoch:15, cur blue:0.4035, best epoch:4, best blue:0.4338\n",
            "cur epoch:16, cur blue:0.4193, best epoch:4, best blue:0.4338\n",
            "cur epoch:17, cur blue:0.4162, best epoch:4, best blue:0.4338\n",
            "cur epoch:18, cur blue:0.4215, best epoch:4, best blue:0.4338\n",
            "cur epoch:19, cur blue:0.4125, best epoch:4, best blue:0.4338\n",
            "cur epoch:20, cur blue:0.4275, best epoch:4, best blue:0.4338\n",
            "cur epoch:21, cur blue:0.4168, best epoch:4, best blue:0.4338\n",
            "cur epoch:22, cur blue:0.4197, best epoch:4, best blue:0.4338\n",
            "cur epoch:23, cur blue:0.4173, best epoch:4, best blue:0.4338\n",
            "cur epoch:24, cur blue:0.4199, best epoch:4, best blue:0.4338\n",
            "cur epoch:25, cur blue:0.4201, best epoch:4, best blue:0.4338\n",
            "cur epoch:26, cur blue:0.4247, best epoch:4, best blue:0.4338\n",
            "cur epoch:27, cur blue:0.4258, best epoch:4, best blue:0.4338\n",
            "cur epoch:28, cur blue:0.4260, best epoch:4, best blue:0.4338\n",
            "cur epoch:29, cur blue:0.4217, best epoch:4, best blue:0.4338\n",
            "cur epoch:30, cur blue:0.4217, best epoch:4, best blue:0.4338\n",
            "cur epoch:31, cur blue:0.4322, best epoch:4, best blue:0.4338\n",
            "cur epoch:32, cur blue:0.4297, best epoch:4, best blue:0.4338\n",
            "cur epoch:33, cur blue:0.4296, best epoch:4, best blue:0.4338\n",
            "cur epoch:34, cur blue:0.4276, best epoch:4, best blue:0.4338\n",
            "cur epoch:35, cur blue:0.4309, best epoch:4, best blue:0.4338\n",
            "cur epoch:36, cur blue:0.4271, best epoch:4, best blue:0.4338\n",
            "cur epoch:37, cur blue:0.4197, best epoch:4, best blue:0.4338\n",
            "cur epoch:38, cur blue:0.4206, best epoch:4, best blue:0.4338\n",
            "cur epoch:39, cur blue:0.4317, best epoch:4, best blue:0.4338\n",
            "cur epoch:40, cur blue:0.4237, best epoch:4, best blue:0.4338\n",
            "cur epoch:41, cur blue:0.4257, best epoch:4, best blue:0.4338\n",
            "cur epoch:42, cur blue:0.4105, best epoch:4, best blue:0.4338\n",
            "cur epoch:43, cur blue:0.4197, best epoch:4, best blue:0.4338\n",
            "cur epoch:44, cur blue:0.4146, best epoch:4, best blue:0.4338\n",
            "cur epoch:45, cur blue:0.4316, best epoch:4, best blue:0.4338\n",
            "cur epoch:46, cur blue:0.4254, best epoch:4, best blue:0.4338\n",
            "cur epoch:47, cur blue:0.4347, best epoch:47, best blue:0.4347\n",
            "cur epoch:48, cur blue:0.4287, best epoch:47, best blue:0.4347\n",
            "cur epoch:49, cur blue:0.4325, best epoch:47, best blue:0.4347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br-O25sNdn0Z",
        "outputId": "f13fad0a-fcd7-4521-bdc9-ea5034c7423a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "model_s = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens).to(device)\n",
        "adam_optimizer = torch.optim.Adam(model_s.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model_t.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL_LS1.pth'))\n",
        "\n",
        "T = 4.7\n",
        "criterionKD = SoftTarget(T)\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(50):\n",
        "    #train(train_loader, model, criterion, epoch)\n",
        "    train_sd(train_loader, model_t, model_s, criterion, criterionKD, transformer_optimizer, epoch)\n",
        "    blue_score = valid (test_loader, model_s)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model_s, 'transformer_optimizer': transformer_optimizer}\n",
        "        #torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object.pth.tar')\n",
        "        torch.save(model_s.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_sd_LS.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.01131, best epoch:0, best blue:0.01131\n",
            "cur epoch:1, cur blue:0.12386, best epoch:1, best blue:0.12386\n",
            "cur epoch:2, cur blue:0.12234, best epoch:1, best blue:0.12386\n",
            "cur epoch:3, cur blue:0.17867, best epoch:3, best blue:0.17867\n",
            "cur epoch:4, cur blue:0.21024, best epoch:4, best blue:0.21024\n",
            "cur epoch:5, cur blue:0.23890, best epoch:5, best blue:0.23890\n",
            "cur epoch:6, cur blue:0.28158, best epoch:6, best blue:0.28158\n",
            "cur epoch:7, cur blue:0.31324, best epoch:7, best blue:0.31324\n",
            "cur epoch:8, cur blue:0.32931, best epoch:8, best blue:0.32931\n",
            "cur epoch:9, cur blue:0.33611, best epoch:9, best blue:0.33611\n",
            "cur epoch:10, cur blue:0.33080, best epoch:9, best blue:0.33611\n",
            "cur epoch:11, cur blue:0.34097, best epoch:11, best blue:0.34097\n",
            "cur epoch:12, cur blue:0.33120, best epoch:11, best blue:0.34097\n",
            "cur epoch:13, cur blue:0.35242, best epoch:13, best blue:0.35242\n",
            "cur epoch:14, cur blue:0.34775, best epoch:13, best blue:0.35242\n",
            "cur epoch:15, cur blue:0.34034, best epoch:13, best blue:0.35242\n",
            "cur epoch:16, cur blue:0.33514, best epoch:13, best blue:0.35242\n",
            "cur epoch:17, cur blue:0.36260, best epoch:17, best blue:0.36260\n",
            "cur epoch:18, cur blue:0.36854, best epoch:18, best blue:0.36854\n",
            "cur epoch:19, cur blue:0.39484, best epoch:19, best blue:0.39484\n",
            "cur epoch:20, cur blue:0.37318, best epoch:19, best blue:0.39484\n",
            "cur epoch:21, cur blue:0.39945, best epoch:21, best blue:0.39945\n",
            "cur epoch:22, cur blue:0.41286, best epoch:22, best blue:0.41286\n",
            "cur epoch:23, cur blue:0.39919, best epoch:22, best blue:0.41286\n",
            "cur epoch:24, cur blue:0.40916, best epoch:22, best blue:0.41286\n",
            "cur epoch:25, cur blue:0.40571, best epoch:22, best blue:0.41286\n",
            "cur epoch:26, cur blue:0.40930, best epoch:22, best blue:0.41286\n",
            "cur epoch:27, cur blue:0.43358, best epoch:27, best blue:0.43358\n",
            "cur epoch:28, cur blue:0.42640, best epoch:27, best blue:0.43358\n",
            "cur epoch:29, cur blue:0.42046, best epoch:27, best blue:0.43358\n",
            "cur epoch:30, cur blue:0.43647, best epoch:30, best blue:0.43647\n",
            "cur epoch:31, cur blue:0.42083, best epoch:30, best blue:0.43647\n",
            "cur epoch:32, cur blue:0.42256, best epoch:30, best blue:0.43647\n",
            "cur epoch:33, cur blue:0.42372, best epoch:30, best blue:0.43647\n",
            "cur epoch:34, cur blue:0.43215, best epoch:30, best blue:0.43647\n",
            "cur epoch:35, cur blue:0.43620, best epoch:30, best blue:0.43647\n",
            "cur epoch:36, cur blue:0.43709, best epoch:36, best blue:0.43709\n",
            "cur epoch:37, cur blue:0.42968, best epoch:36, best blue:0.43709\n",
            "cur epoch:38, cur blue:0.44739, best epoch:38, best blue:0.44739\n",
            "cur epoch:39, cur blue:0.42781, best epoch:38, best blue:0.44739\n",
            "cur epoch:40, cur blue:0.43158, best epoch:38, best blue:0.44739\n",
            "cur epoch:41, cur blue:0.43612, best epoch:38, best blue:0.44739\n",
            "cur epoch:42, cur blue:0.42521, best epoch:38, best blue:0.44739\n",
            "cur epoch:43, cur blue:0.43504, best epoch:38, best blue:0.44739\n",
            "cur epoch:44, cur blue:0.43935, best epoch:38, best blue:0.44739\n",
            "cur epoch:45, cur blue:0.43365, best epoch:38, best blue:0.44739\n",
            "cur epoch:46, cur blue:0.43617, best epoch:38, best blue:0.44739\n",
            "cur epoch:47, cur blue:0.43444, best epoch:38, best blue:0.44739\n",
            "cur epoch:48, cur blue:0.43086, best epoch:38, best blue:0.44739\n",
            "cur epoch:49, cur blue:0.42687, best epoch:38, best blue:0.44739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVMlfkGMgdN6",
        "outputId": "945ab4dd-ff27-445b-ee5b-de1ff9ff8996"
      },
      "source": [
        "model_s.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_sd_LS.pth'))\n",
        "evaluate_matrics(model_s,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.44739445198920014 Precision: 0.5505659667321339 Recall: 0.5133109847480615 F1_Score: 0.5261312770318944 Meteor: 0.4402614129920723 PPL: 5.84861907501931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17881974577903748"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdfMMFLdkXtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218159f8-12db-430c-dc5f-384e3eb08d38"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb  2 00:17:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1mRFSJQGjuE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}